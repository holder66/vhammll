classify a query case using a labeled set of training cases. See the glossary for terminology.
Parameters: A (number of attributes to be used for classification)
	B (binning for continuous attributes)

Description of Classification algorithm:

FUNCTION make a classifier (training dataset; parameter values) returns a trained classifier
	SORT the attributes by rank value using the FUNCTION rank (see above) for the binning specified by the binning parameter
	SELECT the number of highest-ranking attributes as specified by the attributes parameter
	RETURN the bin values for the selected attributes, along with the binning specifications, value maps for discrete attributes, and 
		the assigned class for each case

FUNCTION classify (trained classifier; case to be classified) returns the inferred class
	SELECT the attribute values from the case to be classified according to the trained classifierâ€™s attribute list
	MAP those selected attribute values into bins according to the binning specifications for  continuous attributes or
		value maps for discrete attributes
	FOR EACH case in the trained classifier
		FOR EACH bin value in each case
			CALCULATE  Hamming distance for each bin value to the corresponding bin values for the case to be classified
		SUM the Hamming distances for each case
	SELECT the unique Hamming distances  in the list
	SORT the unique Hamming distances, from minimum to maximum
	FOR EACH unique sorted Hamming distance
		FOR EACH case in the trained classifier whose Hamming distance to the case to be classified equals that Hamming distance
			FOR EACH class
				ACCUMULATE counts of cases by class
		IF the list of case counts has a single MAXIMUM
			RETURN as the inferred class, the class having the MAXIMUM counts
		ELSE  continue the loop
	RETURN an error message (ie if no single maximum could be found when counting over all the possible Hamming distances )



What does binning do: increases "contrast"; makes for a smaller and faster classifier; reduces risk of overfitting.
what does ranking of attributes do: by not using the low-ranked attributes (which contribute noise rather than useful information), the classifier avoids the accuracy degradation due to the "curse of dimensionality". It also contributes to a smaller and faster classifier.
What does the identification of ties in smallest Hamming distance accomplish: it eliminates needing to specify a value for K in a K-nearest neighbors (KNN) algorithm.
Why Hamming distance: eliminates the difficulty most classifiers have with linearly non-separable classes (probably most biological processes)