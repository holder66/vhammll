## Finding useful data in the noise
Much real-world data consists of mostly noise, and only a small amount of information. The HamNN algorithm can be helpful in extracting the useful information.

An example is genomics data. The prostata dataset consists of genomic
microarray data from 102 subjects. Fifty of the 102 subjects were normal,
while the other 52 had a prostate tumor. For each case, there 
are 12534 data points, of which one is the class attribute. Can this
mass of data be used to predict whether a person has a tumor or is normal?

To try the example on your own installation, start by compiling an optimized version, `v -prod .`

Note: processing times are on a 2019 Macbook Pro.

```sh
./vhamml rank -w -wr -s datasets/prostata.tab 
```   
```sh

Attributes Sorted by Rank Value, for "datasets/prostata.tab"
Missing values: included
Bin range for continuous attributes: from 1 to 16 with interval 1
Weighted by class prevalences
         Name                         Index  Type   Rank Value   Bins
     1   NELL2                        10426  C           80.15      9
     2   HPN                           6117  C           78.15      7
     3   PTGDS                         9104  C           74.62      6
     4   RBP1                          6394  C           74.23     13
     5   CALM1_4                      10070  C           72.85     13
     6   HSPD1                         8897  C           72.77     13
     7   TRGC2                         4297  C           70.85     11
     8   TGFB3                        10888  C           70.69     13
     9   DF                            9782  C           70.62     15
    10   SERPINF1                      7179  C           70.38     12
    11   PDLIM5_4                      8782  C           69.23     12
    12   CRYAB_2                       7489  C           68.85      5
	...
	...
	...
 12528   MR1_3                         2048  C            8.77     16
 12529   CPA2                          4056  C            8.54     13
 12530   SEDLP                         5692  C            8.08     14
 12531   PIP                           4163  C            2.00      2
 12532   SEMG2                         4852  C            2.00      2
 12533   SEMG1                         7529  C            2.00      2
processing time: 0 hrs 0 min  1.518 sec

```

Accumulated experience with the algorithm suggests that using a fixed number
of bins often gives good results. 
```sh
./vhamml rank -w -wr -s -b 6 datasets/prostata.tab
```
```sh
Attributes Sorted by Rank Value, for "datasets/prostata.tab"
Missing values: included
Bin range for continuous attributes: from 1 to 6 with interval 1
Weighted by class prevalences
         Name                         Index  Type   Rank Value   Bins
     1   PTGDS                         9104  C           74.62      6
     2   HPN                           6117  C           74.54      5
     3   HSPD1                         8897  C           71.08      2
     4   PDLIM5_4                      8782  C           69.15      4
     5   CRYAB_2                       7489  C           68.85      5
   ...
   ...
   ...
 12530   SEMG1                         7529  C            2.00      2
 12531   AKR1B1                        8486  C            2.00      2
 12532   325_s_at                     12313  C            2.00      2
 12533   PIN1L                          410  C            2.00      3
processing time: 0 hrs 0 min  0.724 sec
```

We can verify this by comparing cross-validation results over a range of 
attribute numbers, either with a fixed number of bins, or with bin number
which is optimized over a range.

Here are the results for exploring over a range of attributes from 1 to 20, 
and a binning range from 2 to 12:
```sh
./vhamml explore -e -g -w -wr -b 2,12 -a 1,20 datasets/prostata.tab
```
Note the flags: -c calls for parallel processing, using all available CPU cores
on you machine; -e for expanded output to the console; -g results in graphical
plots of the results as ROC curves.
```sh

Explore leave-one-out cross-validation using classifiers from "datasets/prostata.tab"
Binning range for continuous attributes: from 2 to 12 with interval 1
Missing values: included
Weighting nearest neighbor counts by class prevalences
Over attribute range from 1 to 20 by interval 1
A correct classification to "normal" is a True Positive (TP);
A correct classification to "tumor" is a True Negative (TN).
Note: for binary classification, balanced accuracy = (sensitivity + specificity) / 2
Attributes    Bins     TP    FP    TN    FN  Sens'y Spec'y PPV    NPV    F1 Score  Raw Acc'y  Bal'd
         1       2     49     1    37    15  0.766  0.974  0.980  0.712  0.860      84.31%   86.97%
         1  2 - 3      48     2    37    15  0.762  0.949  0.960  0.712  0.850      83.33%   85.53%
         1  2 - 4      46     4    31    21  0.687  0.886  0.920  0.596  0.786      75.49%   78.61%
         1  2 - 5      43     7    43     9  0.827  0.860  0.860  0.827  0.843      84.31%   84.35%
         1  2 - 6      35    15    40    12  0.745  0.727  0.700  0.769  0.722      73.53%   73.60%
         1  2 - 7      41     9    49     3  0.932  0.845  0.820  0.942  0.872      88.24%   88.83%
         1  2 - 8      41     9    49     3  0.932  0.845  0.820  0.942  0.872      88.24%   88.83%
         1  2 - 9      36    14    50     2  0.947  0.781  0.720  0.962  0.818      84.31%   86.43%
         1  2 - 10     36    14    50     2  0.947  0.781  0.720  0.962  0.818      84.31%   86.43%
         1  2 - 11     35    15    50     2  0.946  0.769  0.700  0.962  0.805      83.33%   85.76%
         1  2 - 12     35    15    50     2  0.946  0.769  0.700  0.962  0.805      83.33%   85.76%
         2       2     44     6    38    14  0.759  0.864  0.880  0.731  0.815      80.39%   81.11%
         2  2 - 3      46     4    38    14  0.767  0.905  0.920  0.731  0.836      82.35%   83.57%
         2  2 - 4      46     4    43     9  0.836  0.915  0.920  0.827  0.876      87.25%   87.56%
         2  2 - 5      46     4    37    15  0.754  0.902  0.920  0.712  0.829      81.37%   82.83%
         2  2 - 6      44     6    42    10  0.815  0.875  0.880  0.808  0.846      84.31%   84.49%
         2  2 - 7      46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
         2  2 - 8      45     5    45     7  0.865  0.900  0.900  0.865  0.882      88.24%   88.27%
         2  2 - 9      45     5    49     3  0.938  0.907  0.900  0.942  0.918      92.16%   92.25%
         2  2 - 10     45     5    49     3  0.938  0.907  0.900  0.942  0.918      92.16%   92.25%
         2  2 - 11     45     5    49     3  0.938  0.907  0.900  0.942  0.918      92.16%   92.25%
         2  2 - 12     45     5    49     3  0.938  0.907  0.900  0.942  0.918      92.16%   92.25%
         3       2     47     3    39    13  0.783  0.929  0.940  0.750  0.855      84.31%   85.60%
         3  2 - 3      47     3    37    15  0.758  0.925  0.940  0.712  0.839      82.35%   84.15%
         3  2 - 4      47     3    41    11  0.810  0.932  0.940  0.788  0.870      86.27%   87.11%
         3  2 - 5      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         3  2 - 6      49     1    45     7  0.875  0.978  0.980  0.865  0.925      92.16%   92.66%
         3  2 - 7      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
         3  2 - 8      47     3    49     3  0.940  0.942  0.940  0.942  0.940      94.12%   94.12%
         3  2 - 9      47     3    49     3  0.940  0.942  0.940  0.942  0.940      94.12%   94.12%
         3  2 - 10     47     3    49     3  0.940  0.942  0.940  0.942  0.940      94.12%   94.12%
         3  2 - 11     47     3    49     3  0.940  0.942  0.940  0.942  0.940      94.12%   94.12%
         3  2 - 12     47     3    49     3  0.940  0.942  0.940  0.942  0.940      94.12%   94.12%
         4       2     47     3    40    12  0.797  0.930  0.940  0.769  0.862      85.29%   86.34%
         4  2 - 3      39    11    40    12  0.765  0.784  0.780  0.769  0.772      77.45%   77.45%
         4  2 - 4      48     2    42    10  0.828  0.955  0.960  0.808  0.889      88.24%   89.11%
         4  2 - 5      45     5    42    10  0.818  0.894  0.900  0.808  0.857      85.29%   85.59%
         4  2 - 6      48     2    45     7  0.873  0.957  0.960  0.865  0.914      91.18%   91.51%
         4  2 - 7      47     3    47     5  0.904  0.940  0.940  0.904  0.922      92.16%   92.19%
         4  2 - 8      47     3    47     5  0.904  0.940  0.940  0.904  0.922      92.16%   92.19%
         4  2 - 9      47     3    48     4  0.922  0.941  0.940  0.923  0.931      93.14%   93.14%
         4  2 - 10     47     3    48     4  0.922  0.941  0.940  0.923  0.931      93.14%   93.14%
         4  2 - 11     48     2    48     4  0.923  0.960  0.960  0.923  0.941      94.12%   94.15%
         4  2 - 12     48     2    48     4  0.923  0.960  0.960  0.923  0.941      94.12%   94.15%
         5       2     46     4    43     9  0.836  0.915  0.920  0.827  0.876      87.25%   87.56%
         5  2 - 3      40    10    41    11  0.784  0.804  0.800  0.788  0.792      79.41%   79.41%
         5  2 - 4      44     6    42    10  0.815  0.875  0.880  0.808  0.846      84.31%   84.49%
         5  2 - 5      45     5    41    11  0.804  0.891  0.900  0.788  0.849      84.31%   84.74%
         5  2 - 6      47     3    45     7  0.870  0.938  0.940  0.865  0.904      90.20%   90.39%
         5  2 - 7      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
         5  2 - 8      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
         5  2 - 9      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         5  2 - 10     47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         5  2 - 11     46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
         5  2 - 12     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         6       2     48     2    44     8  0.857  0.957  0.960  0.846  0.906      90.20%   90.68%
         6  2 - 3      41     9    43     9  0.820  0.827  0.820  0.827  0.820      82.35%   82.35%
         6  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
         6  2 - 5      47     3    39    13  0.783  0.929  0.940  0.750  0.855      84.31%   85.60%
         6  2 - 6      47     3    43     9  0.839  0.935  0.940  0.827  0.887      88.24%   88.70%
         6  2 - 7      44     6    47     5  0.898  0.887  0.880  0.904  0.889      89.22%   89.24%
         6  2 - 8      44     6    47     5  0.898  0.887  0.880  0.904  0.889      89.22%   89.24%
         6  2 - 9      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         6  2 - 10     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         6  2 - 11     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         6  2 - 12     42     8    47     5  0.894  0.855  0.840  0.904  0.866      87.25%   87.41%
         7       2     48     2    43     9  0.842  0.956  0.960  0.827  0.897      89.22%   89.88%
         7  2 - 3      42     8    43     9  0.824  0.843  0.840  0.827  0.832      83.33%   83.33%
         7  2 - 4      45     5    42    10  0.818  0.894  0.900  0.808  0.857      85.29%   85.59%
         7  2 - 5      45     5    41    11  0.804  0.891  0.900  0.788  0.849      84.31%   84.74%
         7  2 - 6      48     2    43     9  0.842  0.956  0.960  0.827  0.897      89.22%   89.88%
         7  2 - 7      46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
         7  2 - 8      46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
         7  2 - 9      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         7  2 - 10     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         7  2 - 11     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         7  2 - 12     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         8       2     45     5    40    12  0.789  0.889  0.900  0.769  0.841      83.33%   83.92%
         8  2 - 3      42     8    44     8  0.840  0.846  0.840  0.846  0.840      84.31%   84.31%
         8  2 - 4      46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
         8  2 - 5      45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
         8  2 - 6      48     2    44     8  0.857  0.957  0.960  0.846  0.906      90.20%   90.68%
         8  2 - 7      46     4    48     4  0.920  0.923  0.920  0.923  0.920      92.16%   92.15%
         8  2 - 8      46     4    48     4  0.920  0.923  0.920  0.923  0.920      92.16%   92.15%
         8  2 - 9      47     3    45     7  0.870  0.938  0.940  0.865  0.904      90.20%   90.39%
         8  2 - 10     47     3    45     7  0.870  0.938  0.940  0.865  0.904      90.20%   90.39%
         8  2 - 11     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         8  2 - 12     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         9       2     45     5    41    11  0.804  0.891  0.900  0.788  0.849      84.31%   84.74%
         9  2 - 3      43     7    44     8  0.843  0.863  0.860  0.846  0.851      85.29%   85.29%
         9  2 - 4      45     5    44     8  0.849  0.898  0.900  0.846  0.874      87.25%   87.35%
         9  2 - 5      43     7    42    10  0.811  0.857  0.860  0.808  0.835      83.33%   83.42%
         9  2 - 6      49     1    46     6  0.891  0.979  0.980  0.885  0.933      93.14%   93.48%
         9  2 - 7      46     4    48     4  0.920  0.923  0.920  0.923  0.920      92.16%   92.15%
         9  2 - 8      46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
         9  2 - 9      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         9  2 - 10     47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         9  2 - 11     47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         9  2 - 12     48     2    46     6  0.889  0.958  0.960  0.885  0.923      92.16%   92.36%
        10       2     46     4    40    12  0.793  0.909  0.920  0.769  0.852      84.31%   85.11%
        10  2 - 3      42     8    45     7  0.857  0.849  0.840  0.865  0.848      85.29%   85.31%
        10  2 - 4      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        10  2 - 5      44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
        10  2 - 6      46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        10  2 - 7      48     2    46     6  0.889  0.958  0.960  0.885  0.923      92.16%   92.36%
        10  2 - 8      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        10  2 - 9      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        10  2 - 10     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        10  2 - 11     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        10  2 - 12     48     2    48     4  0.923  0.960  0.960  0.923  0.941      94.12%   94.15%
        11       2     48     2    40    12  0.800  0.952  0.960  0.769  0.873      86.27%   87.62%
        11  2 - 3      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        11  2 - 4      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        11  2 - 5      44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
        11  2 - 6      44     6    46     6  0.880  0.885  0.880  0.885  0.880      88.24%   88.23%
        11  2 - 7      48     2    48     4  0.923  0.960  0.960  0.923  0.941      94.12%   94.15%
        11  2 - 8      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
        11  2 - 9      46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        11  2 - 10     46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        11  2 - 11     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        11  2 - 12     47     3    48     4  0.922  0.941  0.940  0.923  0.931      93.14%   93.14%
        12       2     48     2    41    11  0.814  0.953  0.960  0.788  0.881      87.25%   88.35%
        12  2 - 3      44     6    44     8  0.846  0.880  0.880  0.846  0.863      86.27%   86.31%
        12  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        12  2 - 5      44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
        12  2 - 6      46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
        12  2 - 7      48     2    44     8  0.857  0.957  0.960  0.846  0.906      90.20%   90.68%
        12  2 - 8      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
        12  2 - 9      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        12  2 - 10     46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        12  2 - 11     45     5    42    10  0.818  0.894  0.900  0.808  0.857      85.29%   85.59%
        12  2 - 12     47     3    43     9  0.839  0.935  0.940  0.827  0.887      88.24%   88.70%
        13       2     48     2    37    15  0.762  0.949  0.960  0.712  0.850      83.33%   85.53%
        13  2 - 3      44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
        13  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        13  2 - 5      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        13  2 - 6      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        13  2 - 7      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        13  2 - 8      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        13  2 - 9      44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
        13  2 - 10     45     5    44     8  0.849  0.898  0.900  0.846  0.874      87.25%   87.35%
        13  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        13  2 - 12     47     3    43     9  0.839  0.935  0.940  0.827  0.887      88.24%   88.70%
        14       2     47     3    41    11  0.810  0.932  0.940  0.788  0.870      86.27%   87.11%
        14  2 - 3      43     7    41    11  0.796  0.854  0.860  0.788  0.827      82.35%   82.52%
        14  2 - 4      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        14  2 - 5      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        14  2 - 6      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
        14  2 - 7      45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
        14  2 - 8      46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
        14  2 - 9      44     6    42    10  0.815  0.875  0.880  0.808  0.846      84.31%   84.49%
        14  2 - 10     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        14  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        14  2 - 12     47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        15       2     47     3    41    11  0.810  0.932  0.940  0.788  0.870      86.27%   87.11%
        15  2 - 3      42     8    42    10  0.808  0.840  0.840  0.808  0.824      82.35%   82.38%
        15  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        15  2 - 5      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        15  2 - 6      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
        15  2 - 7      46     4    48     4  0.920  0.923  0.920  0.923  0.920      92.16%   92.15%
        15  2 - 8      45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
        15  2 - 9      45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        15  2 - 10     46     4    43     9  0.836  0.915  0.920  0.827  0.876      87.25%   87.56%
        15  2 - 11     44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
        15  2 - 12     46     4    43     9  0.836  0.915  0.920  0.827  0.876      87.25%   87.56%
        16       2     47     3    41    11  0.810  0.932  0.940  0.788  0.870      86.27%   87.11%
        16  2 - 3      42     8    43     9  0.824  0.843  0.840  0.827  0.832      83.33%   83.33%
        16  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        16  2 - 5      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
        16  2 - 6      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
        16  2 - 7      46     4    48     4  0.920  0.923  0.920  0.923  0.920      92.16%   92.15%
        16  2 - 8      44     6    47     5  0.898  0.887  0.880  0.904  0.889      89.22%   89.24%
        16  2 - 9      44     6    45     7  0.863  0.882  0.880  0.865  0.871      87.25%   87.25%
        16  2 - 10     46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        16  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        16  2 - 12     45     5    45     7  0.865  0.900  0.900  0.865  0.882      88.24%   88.27%
        17       2     46     4    41    11  0.807  0.911  0.920  0.788  0.860      85.29%   85.91%
        17  2 - 3      41     9    45     7  0.854  0.833  0.820  0.865  0.837      84.31%   84.38%
        17  2 - 4      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        17  2 - 5      45     5    46     6  0.882  0.902  0.900  0.885  0.891      89.22%   89.22%
        17  2 - 6      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
        17  2 - 7      47     3    48     4  0.922  0.941  0.940  0.923  0.931      93.14%   93.14%
        17  2 - 8      45     5    48     4  0.918  0.906  0.900  0.923  0.909      91.18%   91.20%
        17  2 - 9      44     6    46     6  0.880  0.885  0.880  0.885  0.880      88.24%   88.23%
        17  2 - 10     45     5    45     7  0.865  0.900  0.900  0.865  0.882      88.24%   88.27%
        17  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        17  2 - 12     46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        18       2     46     4    42    10  0.821  0.913  0.920  0.808  0.868      86.27%   86.72%
        18  2 - 3      41     9    44     8  0.837  0.830  0.820  0.846  0.828      83.33%   83.35%
        18  2 - 4      46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        18  2 - 5      48     2    44     8  0.857  0.957  0.960  0.846  0.906      90.20%   90.68%
        18  2 - 6      48     2    47     5  0.906  0.959  0.960  0.904  0.932      93.14%   93.24%
        18  2 - 7      47     3    48     4  0.922  0.941  0.940  0.923  0.931      93.14%   93.14%
        18  2 - 8      44     6    48     4  0.917  0.889  0.880  0.923  0.898      90.20%   90.28%
        18  2 - 9      45     5    45     7  0.865  0.900  0.900  0.865  0.882      88.24%   88.27%
        18  2 - 10     46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
        18  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        18  2 - 12     45     5    42    10  0.818  0.894  0.900  0.808  0.857      85.29%   85.59%
        19       2     48     2    43     9  0.842  0.956  0.960  0.827  0.897      89.22%   89.88%
        19  2 - 3      42     8    44     8  0.840  0.846  0.840  0.846  0.840      84.31%   84.31%
        19  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        19  2 - 5      46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        19  2 - 6      47     3    45     7  0.870  0.938  0.940  0.865  0.904      90.20%   90.39%
        19  2 - 7      47     3    47     5  0.904  0.940  0.940  0.904  0.922      92.16%   92.19%
        19  2 - 8      46     4    48     4  0.920  0.923  0.920  0.923  0.920      92.16%   92.15%
        19  2 - 9      46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
        19  2 - 10     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        19  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        19  2 - 12     46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
        20       2     49     1    44     8  0.860  0.978  0.980  0.846  0.916      91.18%   91.87%
        20  2 - 3      42     8    43     9  0.824  0.843  0.840  0.827  0.832      83.33%   83.33%
        20  2 - 4      47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
        20  2 - 5      45     5    41    11  0.804  0.891  0.900  0.788  0.849      84.31%   84.74%
        20  2 - 6      47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
        20  2 - 7      47     3    48     4  0.922  0.941  0.940  0.923  0.931      93.14%   93.14%
        20  2 - 8      46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
        20  2 - 9      45     5    44     8  0.849  0.898  0.900  0.846  0.874      87.25%   87.35%
        20  2 - 10     45     5    44     8  0.849  0.898  0.900  0.846  0.874      87.25%   87.35%
        20  2 - 11     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
        20  2 - 12     44     6    44     8  0.846  0.880  0.880  0.846  0.863      86.27%   86.31%



processing time: 4 hrs 19 min 44.724 sec
```


We can see that the maximum for balanced accuracy is 94.15%, which first occurs 
for 4 attributes and a bin range of 2-11. However, we achieve almost the same
accuracy (ie, 94.12%) with only 3 attributes and a smaller bin range of 2-8. Using more attributes does not
provide any increases in accuracy.

Experience with this algorithm suggests that using the same number of bins for 
all continuous attributes in some cases provides better results. We can try this
with the -u flag, over an abbreviated range of attributes and bins:

```sh
./vhamml explore -e -g -w -wr -b 6,12 -a 3,5 -u datasets/prostata.tab
```
```sh

Explore leave-one-out cross-validation using classifiers from "datasets/prostata.tab"
Binning range for continuous attributes: from 6 to 12 with interval 1
(same number of bins for all continous attributes)
Missing values: included
Weighting nearest neighbor counts by class prevalences
Over attribute range from 3 to 5 by interval 1
A correct classification to "normal" is a True Positive (TP);
A correct classification to "tumor" is a True Negative (TN).
Note: for binary classification, balanced accuracy = (sensitivity + specificity) / 2
Attributes    Bins     TP    FP    TN    FN  Sens'y Spec'y PPV    NPV    F1 Score  Raw Acc'y  Bal'd
         3       6     49     1    47     5  0.907  0.979  0.980  0.904  0.942      94.12%   94.33%
         3       7     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         3       8     47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%
         3       9     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         3      10     45     5    44     8  0.849  0.898  0.900  0.846  0.874      87.25%   87.35%
         3      11     42     8    40    12  0.778  0.833  0.840  0.769  0.808      80.39%   80.56%
         3      12     42     8    40    12  0.778  0.833  0.840  0.769  0.808      80.39%   80.56%
         4       6     46     4    44     8  0.852  0.917  0.920  0.846  0.885      88.24%   88.43%
         4       7     44     6    45     7  0.863  0.882  0.880  0.865  0.871      87.25%   87.25%
         4       8     46     4    47     5  0.902  0.922  0.920  0.904  0.911      91.18%   91.18%
         4       9     43     7    43     9  0.827  0.860  0.860  0.827  0.843      84.31%   84.35%
         4      10     44     6    41    11  0.800  0.872  0.880  0.788  0.838      83.33%   83.62%
         4      11     46     4    45     7  0.868  0.918  0.920  0.865  0.893      89.22%   89.31%
         4      12     44     6    43     9  0.830  0.878  0.880  0.827  0.854      85.29%   85.39%
         5       6     47     3    44     8  0.855  0.936  0.940  0.846  0.895      89.22%   89.54%
         5       7     45     5    47     5  0.900  0.904  0.900  0.904  0.900      90.20%   90.19%
         5       8     42     8    47     5  0.894  0.855  0.840  0.904  0.866      87.25%   87.41%
         5       9     45     5    43     9  0.833  0.896  0.900  0.827  0.865      86.27%   86.46%
         5      10     44     6    39    13  0.772  0.867  0.880  0.750  0.822      81.37%   81.93%
         5      11     46     4    46     6  0.885  0.920  0.920  0.885  0.902      90.20%   90.23%
         5      12     47     3    46     6  0.887  0.939  0.940  0.885  0.913      91.18%   91.28%



processing time: 0 hrs 16 min  2.036 sec
```

It turns out that three attributes and 6 bins for all attributes,
gives us the best balanced accuracy of 94.33%, marginally better than when using a range of bins. 

Using the "cross" command with the -e flag provides additional statistics, including a confusion matrix:
```sh

./vhamml cross -b 6,6 -a 3 -e -w -wr datasets/prostata.tab
```

```sh

Cross-validation of "datasets/prostata.tab"
Partitioning: leave-one-out
Number of attributes: 3
Binning range for continuous attributes: from 6 to 6 with interval 1
Missing values: included
Purging of duplicate instances: false
Prevalence weighting for ranking attributes: true
Prevalence weighting for nearest neighbor counts: true
Add instances to balance class prevalences: false
Results:
    Class                   Instances    True Positives    Precision    Recall    F1 Score
    normal                         50      49 ( 98.00%)        0.907     0.980       0.942
    tumor                          52      47 ( 90.38%)        0.979     0.904       0.940
        Totals                    102      96 (accuracy: raw: 94.12% balanced: 94.19%)
             Macro Averages:                                   0.943     0.942       0.941
          Weighted Averages:                                   0.944     0.941       0.941
A correct classification to "normal" is a True Positive (TP);
A correct classification to "tumor" is a True Negative (TN).
   TP    FN    TN    FP  Sens'y Spec'y    PPV    NPV  F1 Score  Accuracy: Raw  Balanced
   49     1    47     5   0.980  0.904  0.907  0.979     0.942         94.12%    94.19%
Confusion Matrix:
Predicted Classes (columns)       normal      tumor
      Actual Classes (rows)
                     normal           49          1
                      tumor            5         47
processing time: 0 hrs 0 min 33.505 sec

```

Let's stick with these settings, and create a classifier that
can be used for predicting outcomes:

```sh
./vhamml make -a 3 -b 6 -u -c -o ../classifiers/prostata_classifier datasets/prostata.tab
```

Note the use of the -o flag to specify a file where the classifier is to be saved in a file for later use.