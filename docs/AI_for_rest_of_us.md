# AI for the rest of us!
As a clinician or researcher or scientist, or just an intelligent and curious person, are you excited by the prospect of applying artificial intelligence to the problem you’re interested in? And daunted by the complexity and specialized knowledge required to just get started with AI? You’re most definitely not alone!

Consider a common AI task, classification. The basic idea is that if you have data regarding some outcome of interest, for example, which patients infected with COVID-19 will require hospitalization or mechanical intubation, a classifier which has been trained on existing data should be able to predict the outcome for a new case.

There are a considerable number of algorithms currently in use as classifiers. This [Wikipedia page](https://en.wikipedia.org/wiki/Category:Classification_algorithms) provides links to 86 pages, most of which describe individual classification algorithms. The fact that there are so many, likely reflects the reality that most, if not all, of these algorithms have serious limitations, and/or are useful in only a limited subset of problem areas. The latest AI wonder, deep learning, also has its share of problems; have a look at [this article](https://nautil.us/deep-learning-is-hitting-a-wall-14467/).

But is classification really such a complicated issue? Many multi-celled animals have some sort of nervous system whose main purpose appears to be classification: distinguishing food from nonfood, predators from harmless animals, potential mating partners from kin, and so on. These nervous systems have evolved over millions of years and uncountable generations, providing sufficient accuracy and reliability so that the animal can survive and reproduce. And even at the level of individual cells, classification is universal: deciding what external particles to phagocytose, for example.
These biological classifiers are concerned with how well a new instance matches up with the previously learned instances, ie goodness of fit. In the case of individual cells, this is often based on the shape of surface proteins; think of a key fitting a lock.

Only a single learning exposure is often sufficient in biological classifiers. We don’t need to become infected thousands of times with COVID-19 to develop immunity! Once is enough.

And do biological classifiers need exposure to thousands or even millions of cases to learn to distinguish cats from dogs?

One very important challenge for AI classifiers is dealing with biphasic or triphasic systems. For example, we need water to survive and thrive. Too little water and we die; too much and we die also, either from drowning or water intoxication. Exposure to radiation or toxic chemicals damages plants and animals, but in small doses, can be beneficial; this biphasic response is often called hormesis. The responses in one phase of biphasic or triphasic systems may not be “[linearly separable](https://en.wikipedia.org/wiki/Linear_separability)” from responses in another phase. The issue of dealing with linear separability appears to have had a profound influence on AI development; see this wikipedia entry on [Perceptrons](https://en.wikipedia.org/wiki/Perceptron).

It seems to me that there is a huge divide between the capacities of biological classifiers and current AI classifiers. A divide that I believe can be bridged.

After working for a number of years as a computer systems engineer, I returned to school to study medicine. On encountering neurology and neurophysiology, I was fascinated. The brain is simply a machine, and we should be able to mimic its functioning with computers, I concluded (as did many others both before and after!). But looking into the 1980s state of the art in Artificial Neural Networks (ANNs) I came away quite disappointed. Hidden layers, back propagation, Hopfield nets, learning requiring thousands of passes through the training data; these concepts have no grounding in actual neural networks! What were they thinking!

Because real biological systems, having evolved over millions of years, do their job quite well, it is ultimately foolish to believe we can do better than nature. Some of you may remember when mothers were told by the “experts” that formula was better than breast milk. Or when doctors claimed that smoking cigarettes was good for you. Ah, hubris!

So I’ve drawn my inspiration from nature in developing what I believe is a unique machine learning classifier algorithm. An important influence in my trial-and-error process has been [Gerd Sommerhoff](https://en.wikipedia.org/wiki/Gerd_Sommerhoff), whose book “Logic of the Living Brain” showed using circuit diagrams how ensembles of neurons could deal with learning and storing information about the real world, and using that information to act in the world. 

What makes my algorithm unique? The name, VHamML, is short for the programming language "V", “Hamming”, and ML for Machine. As a nearest neighbor classifier, it learns in a single pass through the training data. Being based on neural networks, it does not depend on counting a specific number of nearest neighbors; instead, it looks only at the closest neighbors to an instance being classified (think of a bubble or sphere in 3-dimensional space) and counts the neighbors for each class. The class with the most neighbors is picked as the inferred class for the instance. If there is a tie, the sphere is enlarged to include the next closest neigbors.

“Hamming” refers to [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance), and is chosen because it is easy for digital computers to calculate for binary strings. In this binary world, each possible value for every feature or attribute describing an instance becomes a dimension in a hyperspace, and the above-mentioned sphere can be thought of as a hypersphere in this space. For categorical or ordinal attributes, the number of possible values is typically fairly small; for number values, the number of possible values is deliberately made small by a process of “binning” in which the range of values between the minimum and the maximum for that attribute is sliced up into a number of bins. In this way, an infinite number of values is reduced to a small number of bins, allowing number attributes to be treated the same as categorical attributes.

While this may sound artificial, it is actually how things work in real life. In the brain, sound intensity at a given frequency is coded for by the number of neurons tuned to that frequency which fire; the higher the intensity, the more neurons are activated. And synaptic weights as well as neurotransmitter activity are quantized by the number of receptor molecules interacting with discrete quantities of neurotransmitter molecules at a synapse. 

Besides making computation simpler, the process of binning itself can improve accuracy by enhancing contrast. But, more importantly, by treating all attributes as categorical (“discrete”), it becomes possible to rank-order them in their ability to distinguish between classes. This is essential in some domains (for example, micro-array genetic data) where most of the attributes represent just “noise” and degrade classification accuracy. 

So how is all this going to benefit you, a potential AI user? First, ease of use of the VHamML algorithm. There are really only two parameters that you can adjust: the number of attributes to use, and the number of bins for number (“continous”) attributes to be sliced into. An “explore” tool makes it easy to find good values for those two parameters.

Besides the two parameters, there are also a couple of "switches" that can be set on or off. One switch controls whether or not prevalence weighting is to be applied when counting up nearest neighbors. Another switch controls whether duplicate instances will be deleted or not when training a classifier. A third switch determines whether to take into account missing values when rank ordering attributes.

Second, there is no need for you to preprocess data, to normalize values, or to account for missing data. Binning takes care of preprocessing and normalizing number values, while missing values are gracefully dealt with (and may even contribute to accuracy);

Third, no need to choose different algorithms for binary class problems or multiclass situations, or for dealing with continuous or discrete attributes or any mix;

Fourth, the algorithm will give you more useful responses if you feed it fine grained information. For example, questionnaires often have a single score result, but the single score may account quite imperfectly for the contribution of each individual question to that final score. The VHamML algorithm, if given the responses to the individual questions, will weight individual responses appropriately;

Fifth, issues with possible linear nonseparability (for example with physiologic data where hormesis may play a role) are moot. For example, training the algorithm to perform with 100% accuracy on the XOR (Exclusive Or) function requires only 4 training cases;

Finally, no need to learn to use a whole variety of different pieces of software (or to assemble a team with all the expertise; for example, a [recent article](https://www.sciencedirect.com/science/article/pii/S193131282200049X?via%3Dihub) cited 26 different items of software and algorithms). The VHamML library contains all the functions you are likely to need, and includes a command line application that lets you use those functions without needing to code or learn complicated platforms like MatLab, R, or SciKitLearn.

But if you do have the urge to “roll your own” and write code to use the VHamML library of functions, I think you’ll find [V a most congenial programming language](https://vlang.io): simple, fast, safe, easy to learn, and fun to use!
_
